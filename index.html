<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Criminal Detection</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      background: #f4f4f4;
      margin: 0;
      padding: 20px;
    }
    .btn {
      margin: 10px;
      padding: 10px 20px;
      font-size: 18px;
      cursor: pointer;
      border: none;
      border-radius: 6px;
      background: #333;
      color: white;
    }
    #uploadSection, #detectSection {
      display: none;
      margin-top: 20px;
    }
    #video {
      width: 100%;
      max-width: 600px;
      height: 400px;
      background: black;
      object-fit: cover; /* camera fills box */
    }
    #switchCamera {
      margin-top: 10px;
      padding: 8px 15px;
      font-size: 16px;
      border: none;
      border-radius: 5px;
      background: #444;
      color: white;
      cursor: pointer;
    }
    #logs {
      margin-top: 20px;
      text-align: left;
      max-width: 600px;
      margin-left: auto;
      margin-right: auto;
      background: white;
      padding: 10px;
      border: 1px solid #ddd;
      border-radius: 8px;
      height: 150px;
      overflow-y: auto;
    }
  </style>
</head>
<body>
  <h1>Criminal Detection System</h1>
  <button class="btn" onclick="showSection('upload')">Train</button>
  <button class="btn" onclick="showSection('detect')">Detect</button>

  <!-- Upload section -->
  <div id="uploadSection">
    <h2>Upload Photos</h2>
    <p>Upload Criminal Photos:</p>
    <input type="file" id="criminalUpload" multiple accept="image/*" /><br><br>
    <p>Upload Employee Photos:</p>
    <input type="file" id="employeeUpload" multiple accept="image/*" /><br><br>

    <h2>Upload Sirens</h2>
    <p>Criminal Siren:</p>
    <input type="file" id="criminalSiren" accept="audio/*" /><br><br>
    <p>Employee Siren:</p>
    <input type="file" id="employeeSiren" accept="audio/*" /><br><br>
    <p>Unknown Siren:</p>
    <input type="file" id="unknownSiren" accept="audio/*" /><br><br>

    <button class="btn" onclick="trainModels()">Train Models</button>
    <p id="trainStatus"></p>
  </div>

  <!-- Detect section -->
  <div id="detectSection">
    <h2>Live Camera</h2>
    <video id="video" autoplay muted playsinline></video><br>
    <button id="switchCamera">ðŸ”„ Switch Camera</button>
    <div id="logs"></div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
  <script>
    let criminalImages = [];
    let employeeImages = [];
    let labeledFaceDescriptors = [];
    let faceMatcher;

    let criminalSirenSound, employeeSirenSound, unknownSirenSound;
    let currentFacingMode = "user"; // default to front camera
    let video = document.getElementById("video");

    function showSection(section) {
      document.getElementById("uploadSection").style.display = "none";
      document.getElementById("detectSection").style.display = "none";
      if (section === "upload") {
        document.getElementById("uploadSection").style.display = "block";
      } else {
        document.getElementById("detectSection").style.display = "block";
        startCamera();
        runDetection();
      }
    }

    // Siren upload
    document.getElementById("criminalSiren").addEventListener("change", e => {
      criminalSirenSound = new Audio(URL.createObjectURL(e.target.files[0]));
    });
    document.getElementById("employeeSiren").addEventListener("change", e => {
      employeeSirenSound = new Audio(URL.createObjectURL(e.target.files[0]));
    });
    document.getElementById("unknownSiren").addEventListener("change", e => {
      unknownSirenSound = new Audio(URL.createObjectURL(e.target.files[0]));
    });

    // Camera start
    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: currentFacingMode }
        });
        video.srcObject = stream;
      } catch (err) {
        console.error("Camera error:", err);
        alert("Camera error: " + err.message);
      }
    }

    // Camera switch
    document.getElementById("switchCamera").addEventListener("click", () => {
      currentFacingMode = currentFacingMode === "user" ? "environment" : "user";
      if (video.srcObject) {
        video.srcObject.getTracks().forEach(track => track.stop());
      }
      startCamera();
    });

    // Load models
    Promise.all([
      faceapi.nets.ssdMobilenetv1.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/weights"),
      faceapi.nets.faceRecognitionNet.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/weights"),
      faceapi.nets.faceLandmark68Net.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/weights")
    ]).then(() => console.log("Models loaded"));

    // Train models
    async function trainModels() {
      const criminalFiles = document.getElementById("criminalUpload").files;
      const employeeFiles = document.getElementById("employeeUpload").files;
      if (criminalFiles.length === 0 && employeeFiles.length === 0) {
        alert("Please upload some images first!");
        return;
      }
      document.getElementById("trainStatus").innerText = "Training...";
      labeledFaceDescriptors = [];

      // Criminals
      if (criminalFiles.length > 0) {
        let descriptors = [];
        for (let file of criminalFiles) {
          const img = await faceapi.bufferToImage(file);
          const detection = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor();
          if (detection) descriptors.push(detection.descriptor);
        }
        if (descriptors.length) labeledFaceDescriptors.push(new faceapi.LabeledFaceDescriptors("Criminal", descriptors));
      }

      // Employees
      if (employeeFiles.length > 0) {
        let descriptors = [];
        for (let file of employeeFiles) {
          const img = await faceapi.bufferToImage(file);
          const detection = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor();
          if (detection) descriptors.push(detection.descriptor);
        }
        if (descriptors.length) labeledFaceDescriptors.push(new faceapi.LabeledFaceDescriptors("Employee", descriptors));
      }

      if (labeledFaceDescriptors.length > 0) {
        faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6);
        document.getElementById("trainStatus").innerText = "Training complete!";
      } else {
        document.getElementById("trainStatus").innerText = "No valid faces found!";
      }
    }

    // Detection loop
    async function runDetection() {
      setInterval(async () => {
        if (!faceMatcher) return;
        const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceDescriptors();
        if (detections.length > 0) {
          detections.forEach(detection => {
            const bestMatch = faceMatcher.findBestMatch(detection.descriptor);
            logResult(bestMatch.toString());
          });
        }
      }, 2000);
    }

    // Logs + sirens
    function logResult(text) {
      const logs = document.getElementById("logs");
      const entry = document.createElement("div");
      entry.innerText = new Date().toLocaleTimeString() + " - " + text;
      logs.prepend(entry);

      if (text.includes("Criminal")) {
        if (criminalSirenSound) criminalSirenSound.play();
      } else if (text.includes("Employee")) {
        if (employeeSirenSound) employeeSirenSound.play();
      } else {
        if (unknownSirenSound) unknownSirenSound.play();
      }
    }
  </script>
</body>
</html>
